{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "## Name: Laasya Ojaswini Bulusu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('Bird_Species.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(rescale=(1./255),horizontal_flip=True,shear_range=0.2)\n",
    "test_gen = ImageDataGenerator(rescale=(1./255))  #--> (0 to 255) convert to (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 150 images belonging to 16 classes.\n",
      "Found 157 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "train = train_gen.flow_from_directory('train_data/train_data',\n",
    "                                      target_size=(120, 120),\n",
    "                                      class_mode='categorical', \n",
    "                                      batch_size=8)\n",
    "test = test_gen.flow_from_directory('test_data/test_data',\n",
    "                                    target_size=(120, 120),\n",
    "                                      class_mode='categorical', \n",
    "                                      batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blasti': 0,\n",
       " 'bonegl': 1,\n",
       " 'brhkyt': 2,\n",
       " 'cbrtsh': 3,\n",
       " 'cmnmyn': 4,\n",
       " 'gretit': 5,\n",
       " 'hilpig': 6,\n",
       " 'himbul': 7,\n",
       " 'himgri': 8,\n",
       " 'hsparo': 9,\n",
       " 'indvul': 10,\n",
       " 'jglowl': 11,\n",
       " 'lbicrw': 12,\n",
       " 'mgprob': 13,\n",
       " 'rebimg': 14,\n",
       " 'wcrsrt': 15}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "\n",
    "from tensorflow.keras.layers import Convolution2D,MaxPooling2D,Flatten,Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the seq model\n",
    "model = Sequential()\n",
    "# Adding conv layer with input\n",
    "model.add(Convolution2D(12,(3,3),activation='relu',input_shape=(120, 120, 3)))\n",
    "# Normalizing the conv layer output\n",
    "model.add(BatchNormalization())\n",
    "# Selecting the max values\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Dropping the unwanted 20% of data\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Convolution2D(24,(3,3),activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Convolution2D(36,(3,3),activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "# Hiddern layers\n",
    "model.add(Dense(62,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(24,activation='relu'))\n",
    "# Output layer\n",
    "model.add(Dense(16,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 118, 118, 12)      336       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 118, 118, 12)     48        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 59, 59, 12)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 59, 59, 12)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 57, 57, 24)        2616      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 57, 57, 24)       96        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 28, 28, 24)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 28, 28, 24)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 36)        7812      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 26, 26, 36)       144       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 13, 13, 36)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 13, 13, 36)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6084)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 62)                377270    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 62)               248       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 62)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2016      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 24)                792       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                400       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 391,778\n",
      "Trainable params: 391,510\n",
      "Non-trainable params: 268\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='accuracy',patience=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "19/19 [==============================] - 72s 4s/step - loss: 2.8277 - accuracy: 0.0800 - val_loss: 2.7866 - val_accuracy: 0.0446\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 66s 4s/step - loss: 2.5906 - accuracy: 0.1933 - val_loss: 2.8561 - val_accuracy: 0.1146\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 76s 4s/step - loss: 2.4092 - accuracy: 0.2933 - val_loss: 2.7685 - val_accuracy: 0.1083\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 72s 4s/step - loss: 2.3343 - accuracy: 0.3133 - val_loss: 2.8981 - val_accuracy: 0.0191\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 66s 4s/step - loss: 2.2180 - accuracy: 0.3267 - val_loss: 3.0351 - val_accuracy: 0.0955\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 65s 3s/step - loss: 2.1077 - accuracy: 0.3600 - val_loss: 3.0267 - val_accuracy: 0.0955\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 66s 4s/step - loss: 2.0405 - accuracy: 0.4200 - val_loss: 2.9553 - val_accuracy: 0.1083\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 64s 3s/step - loss: 1.9098 - accuracy: 0.4467 - val_loss: 3.1925 - val_accuracy: 0.1019\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 63s 3s/step - loss: 1.8330 - accuracy: 0.4867 - val_loss: 3.2090 - val_accuracy: 0.0955\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 63s 3s/step - loss: 1.8484 - accuracy: 0.4667 - val_loss: 3.9630 - val_accuracy: 0.0955\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 63s 3s/step - loss: 1.7721 - accuracy: 0.4800 - val_loss: 3.3050 - val_accuracy: 0.0955\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 62s 3s/step - loss: 1.5479 - accuracy: 0.5533 - val_loss: 3.3826 - val_accuracy: 0.1210\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 327s 18s/step - loss: 1.5494 - accuracy: 0.5400 - val_loss: 3.1315 - val_accuracy: 0.1210\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 66s 4s/step - loss: 1.4520 - accuracy: 0.5733 - val_loss: 3.5646 - val_accuracy: 0.1146\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 65s 4s/step - loss: 1.3168 - accuracy: 0.5933 - val_loss: 3.2468 - val_accuracy: 0.1401\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 65s 4s/step - loss: 1.3000 - accuracy: 0.6067 - val_loss: 3.4294 - val_accuracy: 0.1019\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 65s 4s/step - loss: 1.2925 - accuracy: 0.6067 - val_loss: 3.4378 - val_accuracy: 0.1210\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 66s 4s/step - loss: 1.1026 - accuracy: 0.7000 - val_loss: 3.4177 - val_accuracy: 0.1401\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 65s 4s/step - loss: 1.2237 - accuracy: 0.6200 - val_loss: 3.5250 - val_accuracy: 0.1783\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 65s 4s/step - loss: 1.2821 - accuracy: 0.6133 - val_loss: 3.9630 - val_accuracy: 0.1465\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 64s 4s/step - loss: 0.9817 - accuracy: 0.7133 - val_loss: 3.4636 - val_accuracy: 0.1146\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 65s 4s/step - loss: 0.8756 - accuracy: 0.7733 - val_loss: 3.4475 - val_accuracy: 0.1146\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 69s 4s/step - loss: 0.8938 - accuracy: 0.7467 - val_loss: 3.3889 - val_accuracy: 0.1720\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 68s 4s/step - loss: 1.3140 - accuracy: 0.6133 - val_loss: 3.3623 - val_accuracy: 0.1592\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 72s 4s/step - loss: 1.0907 - accuracy: 0.6467 - val_loss: 3.5273 - val_accuracy: 0.1146\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 71s 4s/step - loss: 0.8765 - accuracy: 0.7733 - val_loss: 2.9824 - val_accuracy: 0.1720\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 71s 4s/step - loss: 0.8392 - accuracy: 0.8133 - val_loss: 2.9971 - val_accuracy: 0.1847\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 68s 4s/step - loss: 0.7414 - accuracy: 0.8067 - val_loss: 2.7892 - val_accuracy: 0.2420\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 68s 4s/step - loss: 0.6277 - accuracy: 0.8267 - val_loss: 3.0760 - val_accuracy: 0.2420\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 80s 4s/step - loss: 0.5736 - accuracy: 0.8267 - val_loss: 3.2363 - val_accuracy: 0.2484\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 170s 9s/step - loss: 0.4565 - accuracy: 0.8667 - val_loss: 3.2890 - val_accuracy: 0.1847\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 178s 10s/step - loss: 0.4563 - accuracy: 0.8800 - val_loss: 3.8035 - val_accuracy: 0.2038\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 167s 9s/step - loss: 0.4770 - accuracy: 0.8733 - val_loss: 3.2875 - val_accuracy: 0.2229\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 173s 9s/step - loss: 0.4028 - accuracy: 0.8800 - val_loss: 3.6310 - val_accuracy: 0.1783\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 160s 9s/step - loss: 0.4294 - accuracy: 0.8800 - val_loss: 3.1178 - val_accuracy: 0.2930\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 174s 10s/step - loss: 0.3842 - accuracy: 0.8933 - val_loss: 3.4423 - val_accuracy: 0.1975\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 165s 9s/step - loss: 0.3217 - accuracy: 0.9067 - val_loss: 3.5916 - val_accuracy: 0.2229\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 241s 13s/step - loss: 0.3147 - accuracy: 0.9067 - val_loss: 3.5357 - val_accuracy: 0.2420\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 90s 5s/step - loss: 0.4426 - accuracy: 0.8800 - val_loss: 3.3206 - val_accuracy: 0.2675\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 84s 5s/step - loss: 0.3028 - accuracy: 0.9067 - val_loss: 3.1959 - val_accuracy: 0.2611\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 83s 4s/step - loss: 0.2521 - accuracy: 0.9333 - val_loss: 3.6787 - val_accuracy: 0.2548\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 82s 4s/step - loss: 0.2919 - accuracy: 0.9200 - val_loss: 3.6267 - val_accuracy: 0.2038\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 79s 4s/step - loss: 0.2117 - accuracy: 0.9533 - val_loss: 3.9102 - val_accuracy: 0.2293\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 110s 6s/step - loss: 0.3276 - accuracy: 0.8867 - val_loss: 3.2409 - val_accuracy: 0.2548\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 115s 6s/step - loss: 0.2220 - accuracy: 0.9533 - val_loss: 3.2043 - val_accuracy: 0.2548\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 111s 6s/step - loss: 0.2962 - accuracy: 0.9000 - val_loss: 3.6921 - val_accuracy: 0.2229\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 113s 6s/step - loss: 0.3378 - accuracy: 0.8800 - val_loss: 3.4937 - val_accuracy: 0.2548\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 110s 6s/step - loss: 0.2428 - accuracy: 0.9267 - val_loss: 3.0251 - val_accuracy: 0.2803\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 110s 6s/step - loss: 0.2673 - accuracy: 0.9133 - val_loss: 3.4673 - val_accuracy: 0.2930\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 96s 5s/step - loss: 0.3476 - accuracy: 0.9000 - val_loss: 3.5872 - val_accuracy: 0.2739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x252b0fb4eb0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train,batch_size=8,validation_data=test,epochs=50,callbacks=early_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blasti': 0,\n",
       " 'bonegl': 1,\n",
       " 'brhkyt': 2,\n",
       " 'cbrtsh': 3,\n",
       " 'cmnmyn': 4,\n",
       " 'gretit': 5,\n",
       " 'hilpig': 6,\n",
       " 'himbul': 7,\n",
       " 'himgri': 8,\n",
       " 'hsparo': 9,\n",
       " 'indvul': 10,\n",
       " 'jglowl': 11,\n",
       " 'lbicrw': 12,\n",
       " 'mgprob': 13,\n",
       " 'rebimg': 14,\n",
       " 'wcrsrt': 15}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 340ms/step\n",
      "2\n",
      "brhkyt\n"
     ]
    }
   ],
   "source": [
    "#Testing 1\n",
    "img = image.load_img('test_data/test_data/brhkyt/D72_0479.jpg',target_size=(120,120))\n",
    "img = image.img_to_array(img)\n",
    "img = np.expand_dims(img,axis=0)\n",
    "pred = np.argmax(model.predict(img))\n",
    "print(pred)\n",
    "output = ['blasti', 'bonegl', 'brhkyt', 'cbrtsh','cmnmyn',\n",
    "          'gretit','hilpig','himbul','himgri','hsparo',\n",
    "          'jglowl','lbicrw','mgprob','rebimg','wcrsrt']\n",
    "print(output[pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "8\n",
      "himgri\n"
     ]
    }
   ],
   "source": [
    "#Testing 2\n",
    "img2 = image.load_img('test_data/test_data/bonegl/DSC_4591.jpg',target_size=(120,120))\n",
    "img2 = image.img_to_array(img2)\n",
    "img2 = np.expand_dims(img2,axis=0)\n",
    "pred = np.argmax(model.predict(img2))\n",
    "print(pred)\n",
    "output = ['blasti', 'bonegl', 'brhkyt', 'cbrtsh','cmnmyn',\n",
    "          'gretit','hilpig','himbul','himgri','hsparo',\n",
    "          'jglowl','lbicrw','mgprob','rebimg','wcrsrt']\n",
    "print(output[pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 150 images belonging to 16 classes.\n",
      "Found 157 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "train_gen = ImageDataGenerator(rescale=(1./255),horizontal_flip=True,shear_range=0.2)\n",
    "test_gen= ImageDataGenerator(rescale=(1./255)) \n",
    "trainVGG = train_gen.flow_from_directory('train_data/train_data',\n",
    "                                      target_size=(170, 170),\n",
    "                                      class_mode='categorical', \n",
    "                                      batch_size=10)\n",
    "testVGG = test_gen.flow_from_directory('test_data/test_data',\n",
    "                                    target_size=(170, 170),\n",
    "                                      class_mode='categorical', \n",
    "                                      batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.layers import Dense,Flatten,Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "vgg = VGG16(include_top=False,weights='imagenet',input_shape=(170,170,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg.layers:\n",
    "  layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten()(vgg.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = Dense(16,activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model(inputs=vgg.input,outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 170, 170, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 170, 170, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 170, 170, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 85, 85, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 85, 85, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 85, 85, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 42, 42, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 42, 42, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 42, 42, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 42, 42, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 21, 21, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 21, 21, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 21, 21, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 21, 21, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 10, 10, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 10, 10, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 10, 10, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 10, 10, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 5, 5, 512)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 12800)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                204816    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,919,504\n",
      "Trainable params: 204,816\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-0c5ac5759710>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model1.fit_generator(trainVGG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15/15 [==============================] - 115s 8s/step - loss: 2.8895 - accuracy: 0.2533 - val_loss: 2.6924 - val_accuracy: 0.3185\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 108s 7s/step - loss: 1.0121 - accuracy: 0.7067 - val_loss: 2.4624 - val_accuracy: 0.2866\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 109s 8s/step - loss: 0.5464 - accuracy: 0.8667 - val_loss: 2.5760 - val_accuracy: 0.3185\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 106s 7s/step - loss: 0.2972 - accuracy: 0.9467 - val_loss: 2.5642 - val_accuracy: 0.3503\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 109s 8s/step - loss: 0.2620 - accuracy: 0.9400 - val_loss: 2.6072 - val_accuracy: 0.3376\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 109s 8s/step - loss: 0.1618 - accuracy: 0.9733 - val_loss: 2.5820 - val_accuracy: 0.3631\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 114s 8s/step - loss: 0.1349 - accuracy: 0.9733 - val_loss: 2.7074 - val_accuracy: 0.3885\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 100s 7s/step - loss: 0.1019 - accuracy: 0.9933 - val_loss: 2.5890 - val_accuracy: 0.3822\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 3226s 230s/step - loss: 0.0745 - accuracy: 0.9933 - val_loss: 2.6234 - val_accuracy: 0.4076\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 110s 8s/step - loss: 0.0557 - accuracy: 1.0000 - val_loss: 2.6813 - val_accuracy: 0.3822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x252b3e660a0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit_generator(trainVGG,\n",
    "                    validation_data=testVGG,\n",
    "                    epochs=10,\n",
    "                    steps_per_epoch=len(trainVGG),\n",
    "                    validation_steps=len(testVGG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 430ms/step\n",
      "2\n",
      "brhkyt\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "img1 = image.load_img('test_data/test_data/brhkyt/D72_0479.jpg',target_size=(170,170))\n",
    "img1 = image.img_to_array(img1)\n",
    "img1 = np.expand_dims(img1,axis=0)\n",
    "pred = np.argmax(model1.predict(img1))\n",
    "print(pred)\n",
    "output = ['blasti', 'bonegl', 'brhkyt', 'cbrtsh','cmnmyn',\n",
    "          'gretit','hilpig','himbul','himgri','hsparo',\n",
    "          'jglowl','lbicrw','mgprob','rebimg','wcrsrt']\n",
    "print(output[pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 128ms/step\n",
      "4\n",
      "cmnmyn\n"
     ]
    }
   ],
   "source": [
    "img2 = image.load_img('Downloads/1800.jpg',target_size=(170,170))\n",
    "img2 = image.img_to_array(img2)\n",
    "img2 = np.expand_dims(img2,axis=0)\n",
    "pred = np.argmax(model1.predict(img2))\n",
    "print(pred)\n",
    "output = ['blasti', 'bonegl', 'brhkyt', 'cbrtsh','cmnmyn',\n",
    "          'gretit','hilpig','himbul','himgri','hsparo',\n",
    "          'jglowl','lbicrw','mgprob','rebimg','wcrsrt']\n",
    "print(output[pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
